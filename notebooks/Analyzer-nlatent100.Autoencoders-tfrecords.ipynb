{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "from numpy import linalg as LA\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import sklearn.preprocessing as prep\n",
    "import pickle\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "def min_max_scale(X):\n",
    "    preprocessor = prep.MinMaxScaler().fit(X)\n",
    "    X_scaled = preprocessor.transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_matrix = joblib.load('../scripts/tumor_and_normal_200000_standardized_X.joblib.pickle')\n",
    "y = joblib.load('../scripts/tumor_and_normal_200000_standardized_y.joblib.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 256 \n",
    "HEIGHT = 256\n",
    "DEPTH = 3\n",
    "\n",
    "def standardize_image(f):\n",
    "    standardized = (imread(f) / 255.0).reshape(-1, 256 * 256 * 3)\n",
    "    return standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "#config\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "\n",
    "class VAE(object):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 learning_rate=0.001,\n",
    "                 n_latent=100,\n",
    "                 batch_size=50):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_latent = n_latent\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self._build_network()\n",
    "        self._create_loss_optimizer()\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        #init = tf.initialize_all_variables()\n",
    "        # Launch the session\n",
    "        self.session = tf.InteractiveSession(config=config)\n",
    "        self.session.run(init)\n",
    "        self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    def _build_network(self):\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        dense1 = tf.layers.dense(\n",
    "            activation=tf.nn.elu, inputs=self.x, units=512)\n",
    "        dense2 = tf.layers.dense(\n",
    "            activation=tf.nn.elu, inputs=dense1, units=512)\n",
    "        dense3 = tf.layers.dense(\n",
    "            activation=tf.nn.elu, inputs=dense2, units=512)\n",
    "        dense4 = tf.layers.dense(\n",
    "            activation=None, inputs=dense3, units=self.n_latent * 2)\n",
    "        self.mu = dense4[:, :self.n_latent]\n",
    "        self.sigma = tf.nn.softplus(dense4[:, self.n_latent:])\n",
    "        eps = tf.random_normal(\n",
    "            shape=tf.shape(self.sigma), mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.mu + self.sigma * eps\n",
    "\n",
    "        ddense1 = tf.layers.dense(\n",
    "            activation=tf.nn.elu, inputs=self.z, units=512)\n",
    "        ddense2 = tf.layers.dense(\n",
    "            activation=tf.nn.elu, inputs=ddense1, units=512)\n",
    "        ddense3 = tf.layers.dense(\n",
    "            activation=tf.nn.elu, inputs=ddense2, units=512)\n",
    "\n",
    "        self.reconstructed = tf.layers.dense(\n",
    "            activation=tf.nn.sigmoid, inputs=ddense3, units=self.input_dim)\n",
    "\n",
    "    def _create_loss_optimizer(self):\n",
    "        epsilon = 1e-10\n",
    "        reconstruction_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(epsilon + self.reconstructed) +\n",
    "            (1 - self.x) * tf.log(epsilon + 1 - self.reconstructed),\n",
    "            axis=1)\n",
    "\n",
    "        self.reconstruction_loss = tf.reduce_mean(\n",
    "            reconstruction_loss) / self.batch_size\n",
    "\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + tf.log(epsilon + self.sigma) - tf.square(self.mu) - tf.square(\n",
    "                self.sigma),\n",
    "            axis=1)\n",
    "        latent_loss = tf.reduce_mean(latent_loss) / self.batch_size\n",
    "        self.latent_loss = latent_loss\n",
    "        self.cost = tf.reduce_mean(self.reconstruction_loss + self.latent_loss)\n",
    "        # ADAM optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "    def fit_minibatch(self, batch):\n",
    "        _, cost, reconstruction_loss, latent_loss = self.session.run(\n",
    "            [\n",
    "                self.optimizer, self.cost, self.reconstruction_loss,\n",
    "                self.latent_loss\n",
    "            ],\n",
    "            feed_dict={self.x: batch})\n",
    "        return cost, reconstruction_loss, latent_loss\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.session.run([self.reconstructed], feed_dict={self.x: x})\n",
    "\n",
    "    def decoder(self, z):\n",
    "        return self.session.run([self.reconstructed], feed_dict={self.z: z})\n",
    "\n",
    "    def encoder(self, x):\n",
    "        return self.session.run([self.z], feed_dict={self.x: x})\n",
    "\n",
    "    def save_model(self, checkpoint_path, epoch):\n",
    "        self.saver.save(self.session, checkpoint_path, global_step=epoch)\n",
    "\n",
    "    def load_model(self, checkpoint_dir):\n",
    "        #new_saver = tf.train.import_meta_graph(checkpoint_path)\n",
    "        #new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=checkpoint_dir, latest_filename='checkpoint')\n",
    "        print('loading model: {}'.format(ckpt.model_checkpoint_path))\n",
    "        self.saver.restore(self.session, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "batch_size=32\n",
    "num_epoch=1000\n",
    "n_latent=100\n",
    "    \n",
    "checkpoint_dir = '/Z/personal-folders/interns/saket/vae_checkpoint_histoapath_2000_nlatent100'   \n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "input_dim = IMAGE_CHANNELS*IMAGE_WIDTH*IMAGE_HEIGHT\n",
    "tf.reset_default_graph()\n",
    "#input_dims = input_dim[1]\n",
    "model = VAE(input_dim=input_dim,\n",
    "            learning_rate=learning_rate,\n",
    "            n_latent=n_latent,\n",
    "            batch_size=batch_size)\n",
    "model.load_model(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model: generation\n",
    "%pylab inline\n",
    "# Sample noise vectors from N(0, 1)\n",
    "z = np.random.normal(size=[model.batch_size, model.n_latent])\n",
    "x_generated = model.decoder(z)[0]\n",
    "\n",
    "w = h = 256 \n",
    "n = np.sqrt(model.batch_size).astype(np.int32)\n",
    "I_generated = np.empty((h*n, w*n, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        I_generated[i*h:(i+1)*h, j*w:(j+1)*w, :] = x_generated[i*n+j, :].reshape(w, h, 3)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(I_generated)# cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_matrix[0].reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_sample = np.reshape(master_matrix, (-1, 256*256*3))\n",
    "x_encoded =  model.encoder(x_sample)\n",
    "x_reconstruct = model.reconstruct(x_sample)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(7):\n",
    "\n",
    "    plt.subplot(7, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(256, 256, 3))\n",
    "    plt.title(\"Test input\")\n",
    "    #plt.colorbar()\n",
    "    plt.subplot(7, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[0][i].reshape(256, 256, 3))\n",
    "    plt.title(\"Reconstruction\")\n",
    "    #plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstruct[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a TPOT on these reduced dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tumor_patches_dir = '/Z/personal-folders/interns/saket/histopath_data/baidu_images/test_tumor_level0/level_0/'\n",
    "list_of_tumor_files = list(glob.glob('{}*.png'.format(test_tumor_patches_dir)))\n",
    "\n",
    "test_normal_patches_dir = '/Z/personal-folders/interns/saket/histopath_data/baidu_images/test_normal_level0/level_0/'\n",
    "list_of_normal_files = list(glob.glob('{}*.png'.format(test_normal_patches_dir)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_encoded[0], y,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_valid, y_valid))\n",
    "pipeline_optimizer.export('tpot_exported_pipeline_autoencoder_nlatent100.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "X_test_matrix = []\n",
    "for f in tqdm_notebook(list_of_tumor_files):\n",
    "    standardized = (imread(f)).reshape(-1, 256 * 256 * 3)\n",
    "    X_test_matrix.append(standardized)    \n",
    "    y_test.append(1)\n",
    "\n",
    "for f in tqdm_notebook(list_of_normal_files):\n",
    "    standardized = (imread(f)).reshape(-1, 256 * 256 * 3)\n",
    "    X_test_matrix.append(standardized)    \n",
    "    y_test.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test_matrix[0].reshape( 256 , 256 , 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_sample[i].reshape(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_matrix = np.array(X_test_matrix)\n",
    "y_test = np.array(y_test)\n",
    "x_test_input = np.reshape(X_test_matrix, (-1, 256*256*3))\n",
    "x_test_encoded = model.encoder(x_test_input)[0]\n",
    "print(pipeline_optimizer.score(x_test_encoded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_reconstructed = model.reconstruct(x_test_input)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "for i in range(10):\n",
    "\n",
    "    plt.subplot(10, 2, 2*i + 1)\n",
    "    plt.imshow(x_test_input[i].reshape(256, 256, 3))\n",
    "    plt.title(\"Test input\")\n",
    "    #plt.colorbar()\n",
    "    plt.subplot(10, 2, 2*i + 2)\n",
    "    plt.imshow(x_test_reconstructed[i].reshape(256, 256, 3))\n",
    "    plt.title(\"Reconstruction\")\n",
    "    #plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_gbm = gbm.predict(x_test_input, num_iteration=gbm.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred_test_gbm) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_gbm_bin = [1 if x>0.5 else 0 for x in y_pred_test_gbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_test_gbm_bin, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
