{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "from numpy import linalg as LA\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import sklearn.preprocessing as prep\n",
    "import pickle\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "def min_max_scale(X):\n",
    "    preprocessor = prep.MinMaxScaler().fit(X)\n",
    "    X_scaled = preprocessor.transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 49152)\n",
      "(?, 49152)\n"
     ]
    }
   ],
   "source": [
    "train_tf_records_file = ['/Z/personal-folders/interns/saket/tf-datasets/baidu_images/level0_128px/train-00000-of-00002',\n",
    "                         '/Z/personal-folders/interns/saket/tf-datasets/baidu_images/level0_128px/train-00001-of-00002']\n",
    "\n",
    "validation_tf_records_file = ['/Z/personal-folders/interns/saket/tf-datasets/baidu_images/level0_128px/validation-00000-of-00002',\n",
    "                              '/Z/personal-folders/interns/saket/tf-datasets/baidu_images/level0_128px/validation-00001-of-00002']\n",
    "\n",
    "\n",
    "training_dataset = tf.data.TFRecordDataset(train_tf_records_file)\n",
    "validation_dataset = tf.data.TFRecordDataset(validation_tf_records_file)\n",
    "\"\"\"\n",
    "dataset = dataset.map(_parse_function)  # Using _parse_function from your question.\n",
    "dataset = dataset.batch(20) \n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "    serialized,\n",
    "    features={\n",
    "        'label': tf.FixedLenFeature([], tf.string),\n",
    "        'image': tf.FixedLenFeature([], tf.string)\n",
    "    }\n",
    ")\n",
    "record_image = tf.decode_raw(features['image'], tf.uint8)\n",
    "\n",
    "image = tf.reshape(record_image, [500, 500, 1])\n",
    "label = tf.cast(features['label'], tf.string)\n",
    "min_after_dequeue = 10\n",
    "batch_size = 1000\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "image_batch, label_batch = tf.train.shuffle_batch(\n",
    "    [image, label], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue\n",
    ")\n",
    "\"\"\"\n",
    "WIDTH = 128\n",
    "HEIGHT = 128\n",
    "DEPTH = 3\n",
    "def preprocess(image):\n",
    "    return image/255.0\n",
    "\n",
    "def parser(serialized_example):\n",
    "    \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.FixedLenFeature([], tf.string),\n",
    "            'image/class/label': tf.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.image.decode_jpeg(features['image/encoded'], channels=3)\n",
    "    \n",
    "    #image.set_shape([DEPTH * HEIGHT * WIDTH])\n",
    "    image = tf.reshape(image, (-1, DEPTH * HEIGHT * WIDTH))\n",
    "    print(image.shape)\n",
    "    # Reshape from [depth * height * width] to [depth, height, width].\n",
    "    #image = tf.cast(\n",
    "    #    tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),\n",
    "    #    tf.float32)\n",
    "    image = tf.cast(\n",
    "        image,\n",
    "        tf.float32)\n",
    "    label = tf.cast(features['image/class/label'], tf.int32)\n",
    "\n",
    "    # Custom preprocessing.\n",
    "    image = preprocess(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "batch_size = 32\n",
    "training_dataset = training_dataset.map(\n",
    "    parser, \n",
    "    num_parallel_calls=batch_size)\n",
    "\n",
    "# Potentially shuffle records.\n",
    "# Batch it up.\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "#training_image_batch, training_label_batch = train_iterator.get_next()\n",
    "validation_dataset = validation_dataset.map(\n",
    "    parser, \n",
    "    num_parallel_calls=batch_size)\n",
    "validation_dataset = validation_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                                    training_dataset.output_shapes)\n",
    "training_next_batch = training_iterator.get_next()\n",
    "training_init_op = training_iterator.make_initializer(training_dataset)\n",
    "validation_init_op = training_iterator.make_initializer(validation_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "#config\n",
    "\n",
    "IMAGE_WIDTH = 128 \n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "class VAE(object):\n",
    "    def __init__(self, input_dim, \n",
    "                 learning_rate=0.001, \n",
    "                 n_latent=8, batch_size=50):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_latent = n_latent\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self._build_network()\n",
    "        self._create_loss_optimizer()\n",
    "\n",
    "        \n",
    "        init = tf.global_variables_initializer()        \n",
    "        #init = tf.initialize_all_variables()\n",
    "        # Launch the session\n",
    "        self.session = tf.InteractiveSession(config=config)\n",
    "        self.session.run(init)\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "    \n",
    "    def _build_network(self):        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        dense1 = tf.layers.dense(activation=tf.nn.elu, inputs=self.x, units=256)\n",
    "        dense2 = tf.layers.dense(activation=tf.nn.elu, inputs=dense1, units=256)\n",
    "        dense3 = tf.layers.dense(activation=tf.nn.elu, inputs=dense2, units=256)\n",
    "        dense4 = tf.layers.dense(activation=None, inputs=dense3, units=self.n_latent * 2)\n",
    "        self.mu = dense4[:, :self.n_latent]\n",
    "        self.sigma = tf.nn.softplus(dense4[:, self.n_latent:])\n",
    "        eps = tf.random_normal(shape=tf.shape(self.sigma),\n",
    "                               mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.mu + self.sigma * eps\n",
    "        \n",
    "        ddense1 = tf.layers.dense(activation=tf.nn.elu, inputs=self.z, units=256)\n",
    "        ddense2 = tf.layers.dense(activation=tf.nn.elu, inputs=ddense1, units=256)\n",
    "        ddense3 = tf.layers.dense(activation=tf.nn.elu, inputs=ddense2, units=256)\n",
    "\n",
    "        self.reconstructed = tf.layers.dense(activation=tf.nn.sigmoid, inputs=ddense3,\n",
    "                                            units=self.input_dim)\n",
    "    \n",
    "    def _create_loss_optimizer(self):\n",
    "        #self.reconstruction_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=self.x,\n",
    "        #                                                                  logits=self.reconstructed))\n",
    "        epsilon = 1e-10\n",
    "        reconstruction_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(epsilon+self.reconstructed) + (1-self.x) * tf.log(epsilon+1-self.reconstructed), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.reconstruction_loss = tf.reduce_mean(reconstruction_loss) / self.batch_size\n",
    "        \n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + tf.log(epsilon+self.sigma) - tf.square(self.mu) - tf.square(self.sigma),\n",
    "                                           axis=1)\n",
    "        latent_loss = tf.reduce_mean(latent_loss) / self.batch_size\n",
    "        self.latent_loss = latent_loss\n",
    "        self.cost = tf.reduce_mean(self.reconstruction_loss + self.latent_loss)\n",
    "        # ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)         \n",
    "    \n",
    "    \n",
    "    def fit_minibatch(self, batch):\n",
    "        _, cost, reconstruction_loss, latent_loss = self.session.run([self.optimizer,\n",
    "                                                                            self.cost,\n",
    "                                                                            self.reconstruction_loss,\n",
    "                                                                            self.latent_loss], \n",
    "                                                                           feed_dict = {self.x: batch})\n",
    "        return  cost, reconstruction_loss, latent_loss\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        return self.session.run([self.reconstructed], feed_dict={self.x: x})\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        return self.session.run([self.reconstructed], feed_dict={self.z: z})\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        return self.session.run([self.z], feed_dict={self.x: x})\n",
    "\n",
    "    def save_model(self, checkpoint_path, epoch):\n",
    "        self.saver.save(self.session, checkpoint_path, global_step = epoch)\n",
    "\n",
    "    def load_model(self, checkpoint_path):\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "        print('loading model: {}'.format(ckpt.model_checkpoint_path))\n",
    "        self.saver.restore(self.session, checkpoint_path+'/'+ckpt.model_checkpoint_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-0a6833f698bc>:29: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "[Epoch 0] Loss: 933.9654541015625, Recon loss: 933.4600830078125, Latent loss: 0.5053769946098328\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 5] Loss: 933.1160888671875, Recon loss: 932.6416015625, Latent loss: 0.47447139024734497\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 10] Loss: 933.022216796875, Recon loss: 932.5433349609375, Latent loss: 0.4788907468318939\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 15] Loss: 933.430908203125, Recon loss: 932.9520874023438, Latent loss: 0.4788227677345276\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 20] Loss: 932.9574584960938, Recon loss: 932.4953002929688, Latent loss: 0.46214738488197327\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 25] Loss: 933.1533203125, Recon loss: 932.6728515625, Latent loss: 0.4804626405239105\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 30] Loss: 933.01708984375, Recon loss: 932.5289306640625, Latent loss: 0.4881354868412018\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 35] Loss: 933.1482543945312, Recon loss: 932.6453857421875, Latent loss: 0.5028575658798218\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 40] Loss: 933.0275268554688, Recon loss: 932.5549926757812, Latent loss: 0.4725481867790222\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 45] Loss: 933.113525390625, Recon loss: 932.6284790039062, Latent loss: 0.485031396150589\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 50] Loss: 933.2007446289062, Recon loss: 932.7374267578125, Latent loss: 0.46331360936164856\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 55] Loss: 933.206787109375, Recon loss: 932.7279052734375, Latent loss: 0.4789052903652191\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 60] Loss: 933.0018310546875, Recon loss: 932.5205688476562, Latent loss: 0.4812443256378174\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 65] Loss: 933.0775756835938, Recon loss: 932.5889892578125, Latent loss: 0.488575279712677\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 70] Loss: 933.21337890625, Recon loss: 932.7487182617188, Latent loss: 0.46468329429626465\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 75] Loss: 933.148681640625, Recon loss: 932.6572265625, Latent loss: 0.491454154253006\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n",
      "[Epoch 80] Loss: 933.1532592773438, Recon loss: 932.6845703125, Latent loss: 0.4687102138996124\n",
      "model saved to /Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3 \n",
    "num_epoch = 1000\n",
    "n_latent = 10\n",
    "checkpoint_dir = '/Z/personal-folders/interns/saket/autoencoder_tfrecord_leve0_ckpt'    \n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "input_dim = DEPTH*WIDTH*HEIGHT\n",
    "model = VAE(input_dim=input_dim,\n",
    "            learning_rate=learning_rate,\n",
    "            n_latent=n_latent,\n",
    "            batch_size=batch_size)\n",
    "total_losses = []\n",
    "reconstruction_losses = []\n",
    "latent_losses = []\n",
    "#train_batches_per_epoch = int(np.floor(training_dataset.s.d.data_size/batch_size))\n",
    "sess = model.session\n",
    "for epoch in range(num_epoch):\n",
    "    sess.run(training_init_op)    \n",
    "    while True:\n",
    "        try:\n",
    "            training_image_batch, training_label_batch = sess.run(training_next_batch)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        input_batch = training_image_batch\n",
    "        input_batch = np.reshape(input_batch, (-1, 128*128*3))\n",
    "        #input_batch = np.asarray(input_batch, dtype=np.float32).reshape(-1, 256*256*3)\n",
    "        total_loss, reconstruction_loss, latent_loss = model.fit_minibatch(input_batch)\n",
    "        latent_losses.append(latent_loss)\n",
    "        reconstruction_losses.append(reconstruction_loss)\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('[Epoch {}] Loss: {}, Recon loss: {}, Latent loss: {}'.format(\n",
    "            epoch, total_loss, reconstruction_loss, latent_loss))\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "        model.save_model(checkpoint_path, epoch)\n",
    "        print (\"model saved to {}\".format(checkpoint_path))\n",
    "\n",
    "print('Done!')\n",
    "#return model, reconstruction_losses, latent_losses,  total_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dims = input_dim[0]*input_dim[1]*input_dim[2]\n",
    "input_dims = input_dim[1]\n",
    "num_sample = train_data.shape[0]\n",
    "model, reconstruction_losses, latent_losses,  total_losses = trainer(train_data, input_dims,\n",
    "                learning_rate=1e-4,  batch_size=32,\n",
    "                num_epoch=1000, n_latent=10, \n",
    "                checkpoint_dir='/Z/personal-folders/interns/saket/vae_checkpoint_histoapath_2000')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model: generation\n",
    "%pylab inline\n",
    "# Sample noise vectors from N(0, 1)\n",
    "z = np.random.normal(size=[model.batch_size, model.n_latent])\n",
    "x_generated = model.decoder(z)[0]\n",
    "\n",
    "w = h = 256\n",
    "n = np.sqrt(model.batch_size).astype(np.int32)\n",
    "I_generated = np.empty((h*n, w*n, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        I_generated[i*h:(i+1)*h, j*w:(j+1)*w, :] = x_generated[i*n+j, :].reshape(256, 256, 3)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(I_generated)# cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = mnist.test.next_batch(100)[0]\n",
    "x_reconstruct = model.reconstruct(x_sample)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(7):\n",
    "\n",
    "    plt.subplot(7, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Test input\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(7, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[0][i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu = vae.encoder(x_sample)[0]\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(z_mu[:, 0], z_mu[:, 1], c=np.argmax(y_sample, 1))\n",
    "plt.colorbar()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "const_init_node = tf.constant_initializer(0.)\n",
    "count_variable = tf.get_variable(\"count\", [], initializer=const_init_node)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run([count_variable]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
