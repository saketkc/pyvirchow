{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
    "from tensorflow.python.ops import resources\n",
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "num_steps = 500 # Total steps to train\n",
    "batch_size = 1024 # The number of samples per batch\n",
    "num_classes = 2 # The 10 digits\n",
    "num_features = 46 # Each image is 28x28 pixels\n",
    "num_trees = 100\n",
    "max_nodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Constructing forest with params = \n",
      "INFO:tensorflow:{'num_classes': 2, 'num_splits_to_consider': 10, 'bagged_features': None, 'base_random_seed': 0, 'max_fertile_nodes': 0, 'split_name': 'less_or_equal', 'checkpoint_stats': False, 'regression': False, 'inference_tree_paths': False, 'finish_type': 0, 'split_after_samples': 250, 'pruning_type': 0, 'max_nodes': 1000, 'split_finish_name': 'basic', 'num_output_columns': 3, 'stats_model_type': 0, 'num_outputs': 1, 'split_type': 0, 'bagged_num_features': 46, 'num_trees': 10, 'num_features': 46, 'early_finish_check_every_samples': 0, 'dominate_method': 'bootstrap', 'use_running_stats_method': False, 'param_file': None, 'bagging_fraction': 1.0, 'valid_leaf_threshold': 1, 'initialize_average_splits': False, 'collate_examples': False, 'leaf_model_type': 0, 'prune_every_samples': 0, 'feature_bagging_fraction': 1.0, 'model_name': 'all_dense', 'dominate_fraction': 0.99, 'split_pruning_name': 'none'}\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "# For random forest, labels must be integers (the class id)\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Random Forest Parameters\n",
    "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
    "                                      num_features=num_features,\n",
    "                                      num_trees=num_trees,\n",
    "                                      max_nodes=max_nodes).fill()\n",
    "\n",
    "# Build the Random Forest\n",
    "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "# Get training graph and loss\n",
    "train_op = forest_graph.training_graph(X, Y)\n",
    "loss_op = forest_graph.training_loss(X, Y)\n",
    "\n",
    "# Measure the accuracy\n",
    "infer_op, _, _ = forest_graph.inference_graph(X)\n",
    "correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value) and forest resources\n",
    "init_vars = tf.group(tf.global_variables_initializer(),\n",
    "    resources.initialize_resources(resources.shared_resources()))\n",
    "\n",
    "# Start TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['area', 'bbox_area', 'compactness', 'convex_area', 'eccentricity',\n",
       "       'equivalent_diameter', 'extent', 'fractal_dimension',\n",
       "       'inertia_tensor_eigvals_1', 'inertia_tensor_eigvals_2',\n",
       "       'major_axis_length', 'max_intensity', 'mean_intensity',\n",
       "       'mean_intensity_entire_image', 'minor_axis_length', 'moments_central_1',\n",
       "       'moments_central_10', 'moments_central_11', 'moments_central_12',\n",
       "       'moments_central_13', 'moments_central_14', 'moments_central_15',\n",
       "       'moments_central_16', 'moments_central_2', 'moments_central_3',\n",
       "       'moments_central_4', 'moments_central_5', 'moments_central_6',\n",
       "       'moments_central_7', 'moments_central_8', 'moments_central_9',\n",
       "       'moments_hu_1', 'moments_hu_2', 'moments_hu_3', 'moments_hu_4',\n",
       "       'moments_hu_5', 'moments_hu_6', 'moments_hu_7', 'nuclei',\n",
       "       'nuclei_intensity_over_entire_image', 'orientation', 'perimeter',\n",
       "       'solidity', 'texture', 'total_nuclei_area', 'total_nuclei_area_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('normal_tumor_segmented_df.tsv')\n",
    "FEATURE_KEYS = df.columns\n",
    "FEATURE_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('normal_tumor_segmented_df_with_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_csv(rows_string_tensor):\n",
    "        \"\"\"Takes the string input tensor and returns tuple of (features, labels).\"\"\"\n",
    "        # Last dim is the label.        \n",
    "        num_features = len(FEATURE_KEYS)\n",
    "        num_columns = num_features + 1 \n",
    "        columns = tf.decode_csv(rows_string_tensor,\n",
    "                                record_defaults=[[0.0]] * num_features + [[0]], \n",
    "                                field_delim=',')\n",
    "        #features = dict(zip(FEATURE_KEYS, columns[:num_features]))\n",
    "        #tf.Print('d', columns)\n",
    "        #print(columns)\n",
    "        return tf.stack(columns[:-1]), columns[-1]\n",
    "\n",
    "def input_fn(file_names, batch_size):   \n",
    "\n",
    "    \"\"\"The input_fn.\"\"\"\n",
    "    dataset = tf.data.TextLineDataset(file_names).skip(1)\n",
    "    # Skip the first line (which does not have data).\n",
    "    dataset = dataset.map(_parse_csv)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    #iterator = dataset.make_one_shot_iterator()\n",
    "    #features, labels = iterator.get_next()\n",
    "    #return features, labels\n",
    "    iterator = tf.data.Iterator.from_structure(dataset.output_types,\n",
    "                                               dataset.output_shapes)\n",
    "    next_batch = iterator.get_next()\n",
    "    init_op = iterator.make_initializer(dataset)\n",
    "    \n",
    "    return init_op, next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_init_op, training_next_batch =  input_fn(['normal_tumor_segmented_df_with_label.csv'],\n",
    "                                                  1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: -3.000000, Acc: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4d79ac9c3322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mtraining_features_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_next_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_steps):\n",
    "    sess.run(training_init_op)    \n",
    "    while True:\n",
    "        try:\n",
    "            training_features_batch, training_label_batch = sess.run(training_next_batch)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        _, l = sess.run([train_op, loss_op], \n",
    "                        feed_dict={X: training_features_batch,\n",
    "                                   Y: training_label_batch})\n",
    "    acc = sess.run(accuracy_op, \n",
    "                   feed_dict={X: training_features_batch, \n",
    "                              Y: training_label_batch})\n",
    "    print('Step %i, Loss: %f, Acc: %f' % (epoch, l, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a27e444af9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_normal_segmented_tsv_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/normal_patches_test_segmented/level_0/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnormal_segmented_tsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_segmented_tsv_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/*.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtumor_segmented_tsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtumor_segmented_tsv_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/*.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "normal_segmented_tsv_dir = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/normal_patches_test_segmented/level_0/'\n",
    "tumor_segmented_tsv_dir = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/tumor_patches_test_segmented/level_0/'\n",
    "test_tumor_segmented_tsv_dir = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/tumor_patches_test_segmented/level_0/'\n",
    "test_normal_segmented_tsv_dir = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/normal_patches_test_segmented/level_0/'\n",
    "\n",
    "normal_segmented_tsv = glob.glob(normal_segmented_tsv_dir+'/*.tsv')\n",
    "tumor_segmented_tsv = glob.glob(tumor_segmented_tsv_dir+'/*.tsv')\n",
    "\n",
    "\n",
    "test_normal_segmented_tsv = glob.glob(test_normal_segmented_tsv_dir+'/*.tsv')\n",
    "test_tumor_segmented_tsv = glob.glob(test_tumor_segmented_tsv_dir+'/*.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_csv(rows_string_tensor):\n",
    "        \"\"\"Takes the string input tensor and returns tuple of (features, labels).\"\"\"\n",
    "        # Last dim is the label.\n",
    "        print(rows_string_tensor)\n",
    "        num_features = len(FEATURE_KEYS)\n",
    "        num_columns = num_features \n",
    "        columns = tf.decode_csv(rows_string_tensor,\n",
    "                                record_defaults=[[0.0]] * num_columns, field_delim='\\t')\n",
    "        #features = dict(zip(FEATURE_KEYS, columns[:num_features]))\n",
    "        tf.Print('d', columns)\n",
    "        print(columns)\n",
    "        return tf.stack(columns)\n",
    "\n",
    "def input_fn(file_names, batch_size):   \n",
    "\n",
    "    \"\"\"The input_fn.\"\"\"\n",
    "    dataset = tf.data.TextLineDataset(file_names).skip(1)\n",
    "    # Skip the first line (which does not have data).\n",
    "    labels = [0 if 'normal' in x else 1 for x in file_names]\n",
    "    labels = tf.data.Dataset.from_tensor_slices(labels)\n",
    "   # dataset = dataset.skip(1)\n",
    "    dataset = dataset.map(_parse_csv)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((dataset, labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "    #iterator = tf.data.Iterator.from_structure(dataset.output_types,\n",
    "    #                                           dataset.output_shapes)\n",
    "    #next_batch = iterator.get_next()\n",
    "    #init_op = iterator.make_initializer(dataset)\n",
    "    #return init_op, next_batch\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_init_op, training_next_batch =  input_fn(normal_segmented_tsv+tumor_segmented_tsv,\n",
    "#                                                  1024)\n",
    "features, labels = input_fn(['normal_tumor_segmented_df_with_label.csv'],\n",
    "                                                  1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_init_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "\n",
    "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
    "                                      num_features=num_features,\n",
    "                                      num_trees=num_trees,\n",
    "                                      max_nodes=max_nodes).fill()\n",
    "\n",
    "\n",
    "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "\n",
    "train_op = forest_graph.training_graph(X, Y)\n",
    "loss_op = forest_graph.training_loss(X, Y)\n",
    "\n",
    "\n",
    "infer_op, _, _ = forest_graph.inference_graph(X)\n",
    "correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "init_vars = tf.group(tf.global_variables_initializer(),\n",
    "    resources.initialize_resources(resources.shared_resources()))\n",
    "\n",
    "# Start TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "y = []\n",
    "\n",
    "def load_df(path):\n",
    "    temp_df = pd.read_table(path)\n",
    "    if len(temp_df.index):\n",
    "        return temp_df\n",
    "    return None\n",
    "\n",
    "with tqdm_notebook(total=len(normal_segmented_tsv)) as pbar:\n",
    "    with Pool(processes=32) as p:\n",
    "        for i, temp_df in enumerate(p.imap_unordered(load_df, normal_segmented_tsv)):\n",
    "            if temp_df is not None:\n",
    "                df = pd.concat((df, temp_df))\n",
    "                y.append(0)\n",
    "            pbar.update()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm_notebook(total=len(tumor_segmented_tsv)) as pbar:\n",
    "    with Pool(processes=32) as p:\n",
    "        for i, temp_df in enumerate(p.imap_unordered(load_df, tumor_segmented_tsv)):\n",
    "            if temp_df is not None:\n",
    "                df = pd.concat((df, temp_df))\n",
    "                y.append(1)\n",
    "            pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label = df.copy()\n",
    "df_with_label['label'] = y\n",
    "df_with_label = df_with_label.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_with_label.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label.to_csv('normal_tumor_segmented_df_with_label.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "filenames = normal_segmented_tsv + tumor_segmented_tsv\n",
    "labels = [0 if 'normal' in x else 1 for x in filenames]\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(filenames), tf.constant(labels)))\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "#iterator = training_dataset.make_one_shot_iterator()\n",
    "#next_element = iterator.get_next()\n",
    "\n",
    "training_iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                                    training_dataset.output_shapes)\n",
    "training_next_batch = training_iterator.get_next()\n",
    "training_init_op = training_iterator.make_initializer(training_dataset)\n",
    "\"\"\"\n",
    "num_epoch = 10\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(num_epoch):\n",
    "        sess.run(training_init_op)    \n",
    "        while True:\n",
    "            try:\n",
    "                training_features_batch, training_label_batch = sess.run(training_next_batch)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            input_batch = training_features_batch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(next_element)\n",
    "    while nof_examples > 0:\n",
    "        nof_examples -= 1\n",
    "        try:\n",
    "            data_features, data_labels = sess.run([features, labels])\n",
    "            print(data_features)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_my_csv(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    record_defaults = [[1.0]]*46\n",
    "    decoded = tf.decode_csv(value, record_defaults = record_defaults, field_delim='\\t')     \n",
    "    return tf.stack(decoded)\n",
    "    \"\"\"\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = tf.some_decoder(record_string)\n",
    "    processed_example = some_processing(example)\n",
    "    \n",
    "    return processed_example, label\n",
    "    \"\"\"\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    labels = [0 if 'normal' in x else 1 for x in filenames]\n",
    "    #labels_queue = tf.train.string_input_producer(\n",
    "    #    labels, num_epochs=num_epochs, shuffle=True)\n",
    "    label_fifo = tf.FIFOQueue(len(filenames),tf.int32, shapes=[[]])\n",
    "    lv = tf.constant(labels)\n",
    "\n",
    "    label_enqueue = label_fifo.enqueue_many([lv])\n",
    "    \n",
    "    example = read_my_csv(filename_queue)\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more\n",
    "    #   memory used.\n",
    "    # capacity must be larger than min_after_dequeue and the amount larger\n",
    "    #   determines the maximum we will prefetch.  Recommendation:\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.batch(\n",
    "      [example, label_fifo.dequeue()], batch_size=batch_size, capacity=capacity)\n",
    "      #min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    \n",
    "\n",
    "    return example_batch, label_batch\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_batch, dataset_label = input_pipeline(normal_segmented_tsv+tumor_segmented_tsv, batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def read_row(csv_row):\n",
    "    record_defaults = [[0.0]]*len(COLUMNS)\n",
    "    row = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    return row\n",
    "\n",
    "def input_pipeline(filenames, batch_size):\n",
    "    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\n",
    "    dataset = (tf.contrib.data.TextLineDataset(filenames)\n",
    "               .skip(1)\n",
    "               .map(lambda line: read_row(line))\n",
    "               .shuffle(buffer_size=10)  # Equivalent to min_after_dequeue=10.\n",
    "               .batch(batch_size))\n",
    "    return dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = input_pipeline(normal_segmented_tsv+tumor_segmented_tsv, batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 500 # Total steps to train\n",
    "batch_size = 1024 # The number of samples per batch\n",
    "num_classes = 2 # The 10 digits\n",
    "num_features = 46 # Each image is 28x28 pixels\n",
    "num_trees = 10\n",
    "max_nodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "# For random forest, labels must be integers (the class id)\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Random Forest Parameters\n",
    "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
    "                                      num_features=num_features,\n",
    "                                      num_trees=num_trees,\n",
    "                                      max_nodes=max_nodes).fill()\n",
    "\n",
    "# Build the Random Forest\n",
    "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "# Get training graph and loss\n",
    "train_op = forest_graph.training_graph(X, Y)\n",
    "loss_op = forest_graph.training_loss(X, Y)\n",
    "\n",
    "# Measure the accuracy\n",
    "infer_op, _, _ = forest_graph.inference_graph(X)\n",
    "correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value) and forest resources\n",
    "init_vars = tf.group(tf.global_variables_initializer(),\n",
    "    resources.initialize_resources(resources.shared_resources()))\n",
    "\n",
    "# Start TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                                    training_dataset.output_shapes)\n",
    "training_next_batch = training_iterator.get_next()\n",
    "\n",
    "training_init_op = training_iterator.make_initializer(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for i in range(1, num_steps + 1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "    if i % 50 == 0 or i == 1:\n",
    "        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n",
    "\n",
    "# Test Model\n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(normal_segmented_tsv+tumor_segmented_tsv)\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "record_defaults = [[1.0]]*46\n",
    "\n",
    "labels = [0]* len(normal_segmented_tsv) + [1] * len(tumor_segmented_tsv)\n",
    "\n",
    "# Default values, in case of empty columns. Also specifies the type of the\n",
    "# decoded result.\n",
    "decoded = tf.decode_csv(value, record_defaults = record_defaults, field_delim='\\t')  \n",
    "stacked_cols = tf.stack(decoded) \n",
    "\n",
    "with tf.Session() as session:\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    tf.train.start_queue_runners(session, coord=coordinator)\n",
    "    #print (session.run(name))\n",
    "    coordinator.request_stop()\n",
    "    coordinator.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices((decoded, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_DEFAULTS = [[0.0]]*len(COLUMNS)\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(COLUMNS,fields))\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('label')\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
