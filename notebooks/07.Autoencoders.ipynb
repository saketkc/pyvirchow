{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "from numpy import linalg as LA\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import sklearn.preprocessing as prep\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "def min_max_scale(X):\n",
    "    preprocessor = prep.MinMaxScaler().fit(X)\n",
    "    X_scaled = preprocessor.transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_patches_dir = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/normal_patches_test/level_0/'\n",
    "tumor_patches_dir = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16_patches/tumor_patches_test/level_0/'\n",
    "\n",
    "np.random.seed(42)\n",
    "master_matrix = []\n",
    "label_matrix = []\n",
    "y = []\n",
    "list_of_tumor_files = list(glob.glob('{}*.png'.format(tumor_patches_dir)))\n",
    "list_of_normal_files = list(glob.glob('{}*.png'.format(normal_patches_dir)))\n",
    "y1 = np.repeat(1, len(list_of_tumor_files))\n",
    "y0 = np.repeat(0, len(list_of_normal_files))\n",
    "y = np.concatenate((y1, y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_tumor_files = np.random.choice(list_of_tumor_files, 10000)\n",
    "for f in tqdm_notebook(list_of_tumor_files):\n",
    "    #standardized = (tf.clip_by_value(imread(f),0, 255)/127.5-1).reshape(-1, 256*256*3)\n",
    "    #standardized = (np.clip(imread(f),0, 255)/127.5-1).reshape(-1, 256*256*3)\n",
    "    standardized = (imread(f)/255.0).reshape(-1, 256*256*3)\n",
    "    master_matrix.append(standardized)\n",
    "    label_matrix.append('tumor')\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(master_matrix, 'tumor_200000_standardized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_matrix = []\n",
    "\n",
    "#list_of_normal_files = np.random.choice(list_of_normal_files, 9999)\n",
    "#np.random.shuffle(list_of_normal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488b7c2da3924e1db2094ec60ffd85fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm_notebook(list_of_normal_files):   \n",
    "    #standardized = (tf.clip_by_value(imread(f),0, 255)/127.5-1).reshape(-1, 256*256*3)\n",
    "    #standardized = (np.clip(imread(f),0, 255)/127.5-1).reshape(-1, 256*256*3)\n",
    "    standardized = (imread(f)/255.0).reshape(-1, 256*256*3)\n",
    "    master_matrix.append(standardized)\n",
    "    label_matrix.append('normal')\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal_200000_standardized.pickle']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pickle.dump(master_matrix, open('tumor_and_normal_200000_standardized.pickle', 'wb'))\n",
    "joblib.dump(master_matrix, 'normal_200000_standardized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_matrix = joblib.load('normal_200000_standardized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_matrix = joblib.load('tumor_200000_standardized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normal_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-986b6feb520f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'normal_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "len(normal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "master_matrix = np.array(master_matrix)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = np.asarray(np.array(master_matrix, axis=np.newaxis), dtype=np.float32)\n",
    "train_data = master_matrix\n",
    "input_dim = train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 2}\n",
    ")\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "#config\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "class VAE(object):\n",
    "    def __init__(self, input_dim, \n",
    "                 learning_rate=0.001, \n",
    "                 n_latent=8, batch_size=50):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_latent = n_latent\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self._build_network()\n",
    "        self._create_loss_optimizer()\n",
    "\n",
    "        \n",
    "        init = tf.global_variables_initializer()        \n",
    "        #init = tf.initialize_all_variables()\n",
    "        # Launch the session\n",
    "        self.session = tf.InteractiveSession(config=config)\n",
    "        self.session.run(init)\n",
    "        self.saver = tf.train.Saver(tf.all_variables())\n",
    "        \n",
    "    \n",
    "    def _build_network(self):        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "        dense1 = tf.layers.dense(activation=tf.nn.elu, inputs=self.x, units=256)\n",
    "        dense2 = tf.layers.dense(activation=tf.nn.elu, inputs=dense1, units=256)\n",
    "        dense3 = tf.layers.dense(activation=tf.nn.elu, inputs=dense2, units=256)\n",
    "        dense4 = tf.layers.dense(activation=None, inputs=dense3, units=self.n_latent * 2)\n",
    "        self.mu = dense4[:, :self.n_latent]\n",
    "        self.sigma = tf.nn.softplus(dense4[:, self.n_latent:])\n",
    "        eps = tf.random_normal(shape=tf.shape(self.sigma),\n",
    "                               mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.mu + self.sigma * eps\n",
    "        \n",
    "        ddense1 = tf.layers.dense(activation=tf.nn.elu, inputs=self.z, units=256)\n",
    "        ddense2 = tf.layers.dense(activation=tf.nn.elu, inputs=ddense1, units=256)\n",
    "        ddense3 = tf.layers.dense(activation=tf.nn.elu, inputs=ddense2, units=256)\n",
    "\n",
    "        self.reconstructed = tf.layers.dense(activation=tf.nn.sigmoid, inputs=ddense3,\n",
    "                                            units=self.input_dim)\n",
    "    \n",
    "    def _create_loss_optimizer(self):\n",
    "        #self.reconstruction_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=self.x,\n",
    "        #                                                                  logits=self.reconstructed))\n",
    "        epsilon = 1e-10\n",
    "        reconstruction_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(epsilon+self.reconstructed) + (1-self.x) * tf.log(epsilon+1-self.reconstructed), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.reconstruction_loss = tf.reduce_mean(reconstruction_loss) / self.batch_size\n",
    "        \n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + tf.log(epsilon+self.sigma) - tf.square(self.mu) - tf.square(self.sigma),\n",
    "                                           axis=1)\n",
    "        latent_loss = tf.reduce_mean(latent_loss) / self.batch_size\n",
    "        self.latent_loss = latent_loss\n",
    "        self.cost = tf.reduce_mean(self.reconstruction_loss + self.latent_loss)\n",
    "        # ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)         \n",
    "    \n",
    "    \n",
    "    def fit_minibatch(self, batch):\n",
    "        _, cost, reconstruction_loss, latent_loss = self.session.run([self.optimizer,\n",
    "                                                                            self.cost,\n",
    "                                                                            self.reconstruction_loss,\n",
    "                                                                            self.latent_loss], \n",
    "                                                                           feed_dict = {self.x: batch})\n",
    "        return  cost, reconstruction_loss, latent_loss\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        return self.session.run([self.reconstructed], feed_dict={self.x: x})\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        return self.session.run([self.reconstructed], feed_dict={self.z: z})\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        return self.session.run([self.z], feed_dict={self.x: x})\n",
    "\n",
    "    def save_model(self, checkpoint_path, epoch):\n",
    "        self.saver.save(self.session, checkpoint_path, global_step = epoch)\n",
    "\n",
    "    def load_model(self, checkpoint_path):\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "        print('loading model: {}'.format(ckpt.model_checkpoint_path))\n",
    "        self.saver.restore(self.session, checkpoint_path+'/'+ckpt.model_checkpoint_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(data, input_dim,\n",
    "            learning_rate=1e-3, batch_size=100,\n",
    "            num_epoch=50, n_latent=10, checkpoint_dir='/tmp/vae_checkpoint'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model = VAE(input_dim=input_dim,\n",
    "                learning_rate=learning_rate,\n",
    "                n_latent=n_latent,\n",
    "                batch_size=batch_size)\n",
    "    total_losses = []\n",
    "    reconstruction_losses = []\n",
    "    latent_losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        #for (batch, labels) in iter.get_next():            \n",
    "        #    print(batch)\n",
    "        for iter in range(num_sample // batch_size):\n",
    "            batch = data[iter*batch_size: min((iter+1)*batch_size, data.shape[0]),]\n",
    "            input_batch = batch[0]\n",
    "            #input_batch = tf.reshape(input_batch, (-1, 256*256*3))\n",
    "            #input_batch = np.asarray(input_batch, dtype=np.float32).reshape(-1, 256*256*3)\n",
    "            #print(input_batch.shape)\n",
    "            total_loss, reconstruction_loss, latent_loss = model.fit_minibatch(input_batch)\n",
    "        latent_losses.append(latent_loss)\n",
    "        reconstruction_losses.append(reconstruction_loss)\n",
    "        total_losses.append(total_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('[Epoch {}] Loss: {}, Recon loss: {}, Latent loss: {}'.format(\n",
    "                epoch, total_loss, reconstruction_loss, latent_loss))\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "            model.save_model(checkpoint_path, epoch)\n",
    "            print (\"model saved to {}\".format(checkpoint_path))\n",
    "            \n",
    "    print('Done!')\n",
    "    return model, reconstruction_losses, latent_losses,  total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dims = input_dim[0]*input_dim[1]*input_dim[2]\n",
    "input_dims = input_dim[1]\n",
    "num_sample = train_data.shape[0]\n",
    "model, reconstruction_losses, latent_losses,  total_losses = trainer(train_data, input_dims,\n",
    "                learning_rate=1e-4,  batch_size=32,\n",
    "                num_epoch=1000, n_latent=10, \n",
    "                checkpoint_dir='/Z/personal-folders/interns/saket/vae_checkpoint_histoapath_2000')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model: generation\n",
    "%pylab inline\n",
    "# Sample noise vectors from N(0, 1)\n",
    "z = np.random.normal(size=[model.batch_size, model.n_latent])\n",
    "x_generated = model.decoder(z)[0]\n",
    "\n",
    "w = h = 256\n",
    "n = np.sqrt(model.batch_size).astype(np.int32)\n",
    "I_generated = np.empty((h*n, w*n, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        I_generated[i*h:(i+1)*h, j*w:(j+1)*w, :] = x_generated[i*n+j, :].reshape(256, 256, 3)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(I_generated)# cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = mnist.test.next_batch(100)[0]\n",
    "x_reconstruct = model.reconstruct(x_sample)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(7):\n",
    "\n",
    "    plt.subplot(7, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Test input\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(7, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[0][i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu = vae.encoder(x_sample)[0]\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(z_mu[:, 0], z_mu[:, 1], c=np.argmax(y_sample, 1))\n",
    "plt.colorbar()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "const_init_node = tf.constant_initializer(0.)\n",
    "count_variable = tf.get_variable(\"count\", [], initializer=const_init_node)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run([count_variable]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
