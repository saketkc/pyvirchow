{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Just use 1 GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "from pywsi.io import WSIReader\n",
    "from pywsi.morphology import TissuePatch\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from shapely.geometry import Point as shapelyPoint\n",
    "from shapely.geometry import box as shapelyRectangle\n",
    "\n",
    "from pywsi.io.operations import get_annotation_bounding_boxes, get_annotation_polygons, translate_and_scale_object\n",
    "from pywsi.io.operations import translate_and_scale_polygon\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "from shapely.geometry import Polygon as shapelyPolygon\n",
    "import openslide\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import cv2\n",
    "\n",
    "from pywsi.io.operations import get_annotation_bounding_boxes, get_annotation_polygons, \\\n",
    "    poly2mask, translate_and_scale_polygon, read_as_rgb\n",
    "\n",
    "from pywsi.morphology.patch_extractor import TissuePatch\n",
    "from pywsi.morphology.mask import mpl_polygon_to_shapely_scaled, get_common_interior_polygons\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pywsi.io.tiling import generate_tiles, get_all_patches_from_slide\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 # not_tumor, tumor\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(256, 256, 3)))\n",
    "model.add(Convolution2D(100, (5, 5), strides=(2, 2), activation='elu', padding='same'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Convolution2D(200, (5, 5), strides=(2, 2), activation='elu', padding='same'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Convolution2D(300, (3, 3), activation='elu', padding='same'))\n",
    "model.add(Convolution2D(400, (3, 3), activation='elu',  padding='same'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(400, (3, 3), activation='elu',  padding='same'))\n",
    "model.add(Convolution2D(300, (3, 3), activation='elu',  padding='same'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Convolution2D(2, (1, 1))) # this is called upscore layer for some reason?\n",
    "model.add(Conv2DTranspose(2, (31, 31), strides=(16, 16), activation='softmax', padding='same'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights-improvement-11-0.97.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try on tumor_076 samples\n",
    "def predict_from_model(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, 256, 256, 3))\n",
    "    prediction = prediction[:, :, :, 1].reshape(256, 256)\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_blend(patch, prediction, ax, alpha=0.75):\n",
    "    \"\"\"alpha blend patch and prediction.\n",
    "    https://matplotlib.org/examples/pylab_examples/layer_images.html\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: prediction: 256x256x1, per-pixel tumor probability\n",
    "    input: ax: maplotlib Axes object\n",
    "    input: alpha: alpha blend\n",
    "    \"\"\"\n",
    "    \n",
    "    dx, dy = 0.05, 0.05\n",
    "    x = np.arange(0, patch.shape[1] - 1, dx)\n",
    "    y = np.arange(0, patch.shape[0] - 1, dy)\n",
    "    xmin, xmax, ymin, ymax = np.amin(x), np.amax(x), np.amin(y), np.amax(y)\n",
    "    extent = xmin, xmax, ymin, ymax\n",
    "\n",
    "    # fig = plt.figure(frameon=False, figsize=(10, 5))\n",
    "    Z1 = rgb2gray(patch)\n",
    "    Z2 = prediction\n",
    "\n",
    "    im1 = ax.imshow(Z1, cmap='gray', extent=extent)\n",
    "    im2 = ax.imshow(Z2, cmap='coolwarm', alpha=alpha, vmin=0.0, vmax=1.0,\n",
    "                     extent=extent)\n",
    "    ax.axis('off');\n",
    "\n",
    "def plot_patch_with_pred(patch, truth, prediction, title_str='', alpha=0.6):\n",
    "    \"\"\"\n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: truth: 256x256x2, onehot output classes (not_tumor, tumor)\n",
    "    input: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    gs = gridspec.GridSpec(2, 4, width_ratios=[10, 10, 19, 1])\n",
    "    ax0 = plt.subplot(gs[0, 0])\n",
    "    ax1 = plt.subplot(gs[0, 1])\n",
    "    ax2 = plt.subplot(gs[1, 0])\n",
    "    ax3 = plt.subplot(gs[1, 1])\n",
    "    ax4 = plt.subplot(gs[:, 2])\n",
    "    axc = plt.subplot(gs[:, 3])\n",
    "\n",
    "    ax0.imshow(patch);\n",
    "    ax0.set_title('Original')\n",
    "    \n",
    "    ax1.imshow(truth.argmax(axis=2), cmap='gray', vmin=0, vmax=1);\n",
    "    ax1.set_title('Truth mask (white=tumor, black=not_tumor)')\n",
    "    \n",
    "    p = ax2.imshow(prediction, cmap='coolwarm', vmin=0, vmax=1);\n",
    "    ax2.set_title('Prediction heatmap')\n",
    "\n",
    "    ax3.imshow((prediction > 0.5).astype(np.int), cmap='gray', vmin=0, vmax=1);\n",
    "    ax3.set_title('Prediction mask (white=tumor, black=not_tumor)')\n",
    "    \n",
    "    plot_blend(patch, prediction, ax4, alpha)\n",
    "    ax4.set_title('Original+Prediction blend')\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 10)\n",
    "    fig.suptitle(title_str)\n",
    "    fig.colorbar(p, cax=axc, orientation=\"vertical\")\n",
    "    axc.set_title('Probability pixel is tumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_from_model(patches, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: `batch_size`x256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    predictions = model.predict(patches)\n",
    "    predictions = predictions[:, :, :, 1]\n",
    "    return predictions\n",
    "\"\"\"\n",
    "validation_samples = len(tumor_076.index)\n",
    "validation_generator = generate_tiles(tumor_076, BATCH_SIZE)\n",
    "validation_steps = np.ceil((validation_samples) / BATCH_SIZE)\n",
    "\n",
    "confusion_mtx = np.zeros((2, 2))\n",
    "\n",
    "for i in tqdm_notebook(range(int(validation_steps))):\n",
    "    X, y  = next(validation_generator)\n",
    "    preds = predict_batch_from_model(X, model)\n",
    "    \n",
    "    y_true = y[:, :, :, 1].ravel()\n",
    "    y_pred = np.uint8(preds > 0.5).ravel()\n",
    "    \n",
    "    confusion_mtx += confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_df = pd.read_table('/Z/personal-folders/interns/saket/histopath_data/patches_dataframe/training/tumor/master_df.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_076 = tumor_df[tumor_df.uid=='tumor_076']\n",
    "tumor_082 = tumor_df[tumor_df.uid=='tumor_082']\n",
    "tumor_002 = tumor_df[tumor_df.uid=='tumor_002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_002.tile_loc = [eval(x) for x in tumor_002.tile_loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gen = generate_tiles(tumor_002.sample(32, random_state=42), 32, shuffle=False)\n",
    "\n",
    "example_X, example_y  = next(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_patch = example_X[1]\n",
    "example_truth = example_y[1]\n",
    "\n",
    "prediction = predict_from_model(example_patch, model)\n",
    "plot_patch_with_pred(example_patch, example_truth, prediction, title_str='Example Tumor Patch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Z/personal-folders/interns/saket/histopath_data/prediction_heatmaps/tumor_002'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "alpha = 0.5\n",
    "slide_path = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16/training/tumor/tumor_002.tif'\n",
    "json_filepath = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16/training/lesion_annotations_json/tumor_002.json'\n",
    "all_samples = get_all_patches_from_slide(slide_path, json_filepath, False, 256)\n",
    "slide = WSIReader(slide_path, 40)\n",
    "\n",
    "n_samples = len(all_samples)\n",
    "n_cols = int(slide.dimensions[0] / 256)\n",
    "n_rows = int(slide.dimensions[1] / 256)\n",
    "#assert n_cols * n_rows == n_samples\n",
    "\n",
    "thumbnail = slide.get_thumbnail((n_cols, n_rows))\n",
    "thumbnail = np.array(thumbnail)\n",
    "\n",
    "# batch_size = n_cols\n",
    "batch_size = 32\n",
    "output_thumbnail_preds = list()\n",
    "    \n",
    "for offset in tqdm_notebook(list(range(0, n_samples, batch_size))):\n",
    "    batch_samples = all_samples.iloc[offset:offset+batch_size]\n",
    "    #batch_samples.loc[: 'tile_loc'] = [eval(x) for x in batch_samples.tile_loc]\n",
    "    png_fnames = batch_samples.tile_loc.apply(lambda coord: os.path.join(output_dir,\n",
    "                                                                         '{}_{}.png'.format(coord[1], coord[0])))\n",
    "    \n",
    "    X, _ = next(generate_tiles(batch_samples, batch_size, shuffle=False))\n",
    "    \n",
    "    if batch_samples.is_tissue.nunique() == 1 and batch_samples.iloc[0].is_tissue == False:\n",
    "        # all patches in this row do not have tissue, skip them all\n",
    "        output_thumbnail_preds.append(np.zeros(batch_size, dtype=np.float32))\n",
    "        \n",
    "        # output pngs\n",
    "        for i, png_fname in enumerate(png_fnames):\n",
    "            plt.imsave(png_fname, X[i])\n",
    "    else:\n",
    "        # make predictions\n",
    "        preds = predict_batch_from_model(X, model)\n",
    "        output_thumbnail_preds.append(preds.mean(axis=(1,2)))\n",
    "        \n",
    "        # overlay preds\n",
    "        # save blended imgs\n",
    "        for i, png_fname in enumerate(png_fnames):\n",
    "            pred_i = preds[i]\n",
    "            X_i = X[i]\n",
    "            #output_img = rgb2gray(X_i)\n",
    "            #output_img2 = gray2rgb(output_img.copy())\n",
    "\n",
    "            #overlay = np.uint8(cm.viridis(pred_i) * 255)[:,:,:3]\n",
    "            #blended = overlay*alpha + output_img2 *(1-alpha) +  0\n",
    "            output_img = cv2.cvtColor(X_i, cv2.COLOR_RGB2GRAY)\n",
    "            output_img2 = cv2.cvtColor(output_img.copy(), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            overlay = np.uint8(cm.viridis(pred_i) * 255)[:,:,:3]\n",
    "            blended = cv2.addWeighted(overlay, alpha, output_img2, 1-alpha, 0, output_img)\n",
    "            #blended = overlay*alpha + output_img2 *(1-alpha) +  0\n",
    "            #blended = np.clip(blended, 0, 255)\n",
    "            plt.imsave(png_fname, blended)\n",
    "        \n",
    "\n",
    "output_thumbnail_preds = np.array(output_thumbnail_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first 10 and last 10 entries\n",
    "output_thumbnail_preds_reshaped = output_thumbnail_preds[1:-1,:].reshape(836, 392)#reshape(n_rows, n_cols)\n",
    "f, axes = plt.subplots(1, 2, figsize=(40, 18))\n",
    "ax = axes.flatten()\n",
    "plot_blend(thumbnail, output_thumbnail_preds, ax=ax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Z/personal-folders/interns/saket/histopath_data/prediction_heatmaps/tumor_009'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "alpha = 0.5\n",
    "slide_path = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16/training/tumor/tumor_009.tif'\n",
    "json_filepath = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16/training/lesion_annotations_json/tumor_009.json'\n",
    "all_samples = get_all_patches_from_slide(slide_path, json_filepath, False, 256)\n",
    "slide = WSIReader(slide_path, 40)\n",
    "\n",
    "n_samples = len(all_samples)\n",
    "n_cols = int(slide.dimensions[0] / 256)\n",
    "n_rows = int(slide.dimensions[1] / 256)\n",
    "assert n_cols * n_rows == n_samples\n",
    "\n",
    "thumbnail = slide.get_thumbnail((n_cols, n_rows))\n",
    "thumbnail = np.array(thumbnail)\n",
    "\n",
    "# batch_size = n_cols\n",
    "batch_size = 32\n",
    "output_thumbnail_preds = list()\n",
    "    \n",
    "for offset in tqdm(list(range(0, n_samples, batch_size))):\n",
    "    batch_samples = all_samples.iloc[offset:offset+batch_size]\n",
    "    #batch_samples.loc[: 'tile_loc'] = [eval(x) for x in batch_samples.tile_loc]\n",
    "    png_fnames = batch_samples.tile_loc.apply(lambda coord: os.path.join(output_dir,\n",
    "                                                                         '{}_{}.png'.format(coord[1], coord[0])))\n",
    "    \n",
    "    X, _ = next(generate_tiles(batch_samples, batch_size, shuffle=False))\n",
    "    \n",
    "    if batch_samples.is_tissue.nunique() == 1 and batch_samples.iloc[0].is_tissue == False:\n",
    "        # all patches in this row do not have tissue, skip them all\n",
    "        output_thumbnail_preds.append(np.zeros(batch_size, dtype=np.float32))\n",
    "        \n",
    "        # output pngs\n",
    "        for i, png_fname in enumerate(png_fnames):\n",
    "            plt.imsave(png_fname, X[i])\n",
    "    else:\n",
    "        # make predictions\n",
    "        preds = predict_batch_from_model(X, model)\n",
    "        output_thumbnail_preds.append(preds.mean(axis=(1,2)))\n",
    "        \n",
    "        # overlay preds\n",
    "        # save blended imgs\n",
    "        for i, png_fname in enumerate(png_fnames):\n",
    "            pred_i = preds[i]\n",
    "            X_i = X[i]\n",
    "            #output_img = rgb2gray(X_i)\n",
    "            #output_img2 = gray2rgb(output_img.copy())\n",
    "\n",
    "            #overlay = np.uint8(cm.viridis(pred_i) * 255)[:,:,:3]\n",
    "            #blended = overlay*alpha + output_img2 *(1-alpha) +  0\n",
    "            output_img = cv2.cvtColor(X_i, cv2.COLOR_RGB2GRAY)\n",
    "            output_img2 = cv2.cvtColor(output_img.copy(), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            overlay = np.uint8(cm.viridis(pred_i) * 255)[:,:,:3]\n",
    "            blended = cv2.addWeighted(overlay, alpha, output_img2, 1-alpha, 0, output_img)\n",
    "            #blended = overlay*alpha + output_img2 *(1-alpha) +  0\n",
    "            #blended = np.clip(blended, 0, 255)\n",
    "            plt.imsave(png_fname, blended)\n",
    "        \n",
    "\n",
    "output_thumbnail_preds = np.array(output_thumbnail_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def process_batch(batch_samples):\n",
    "output_dir = '/Z/personal-folders/interns/saket/histopath_data/prediction_heatmaps/tumor_009'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "alpha = 0.5\n",
    "slide_path = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16/training/tumor/tumor_009.tif'\n",
    "json_filepath = '/Z/personal-folders/interns/saket/histopath_data/CAMELYON16/training/lesion_annotations_json/tumor_009.json'\n",
    "all_samples = get_all_patches_from_slide(slide_path, json_filepath, False, 256)\n",
    "slide = WSIReader(slide_path, 40)\n",
    "\n",
    "n_samples = len(all_samples)\n",
    "n_cols = int(slide.dimensions[0] / 256)\n",
    "n_rows = int(slide.dimensions[1] / 256)\n",
    "assert n_cols * n_rows == n_samples\n",
    "\n",
    "thumbnail = slide.get_thumbnail((n_cols, n_rows))\n",
    "thumbnail = np.array(thumbnail)\n",
    "\n",
    "# batch_size = n_cols\n",
    "batch_size = 32\n",
    "output_thumbnail_preds = list()\n",
    "    \n",
    "    \n",
    "def process_batch(args):\n",
    "    idx, batch_samples = args\n",
    "    png_fnames = batch_samples.tile_loc.apply(lambda coord: os.path.join(output_dir,\n",
    "                                                                         '{}_{}.png'.format(coord[1], coord[0])))\n",
    "    output_thumbnail_pred = None\n",
    "    X, _ = next(generate_tiles(batch_samples, batch_size, shuffle=False))\n",
    "    if batch_samples.is_tissue.nunique() == 1 and batch_samples.iloc[0].is_tissue == False:\n",
    "        # all patches in this row do not have tissue, skip them all\n",
    "        output_thumbnail_pred = np.zeros(batch_size, dtype=np.float32)\n",
    "        \n",
    "        # output pngs\n",
    "        for i, png_fname in enumerate(png_fnames):\n",
    "            plt.imsave(png_fname, X[i])\n",
    "    else:\n",
    "        # make predictions\n",
    "        preds = predict_batch_from_model(X, model)\n",
    "        output_thumbnail_pred = preds.mean(axis=(1,2))\n",
    "        \n",
    "        # overlay preds\n",
    "        # save blended imgs\n",
    "        \n",
    "        for i, png_fname in enumerate(png_fnames):\n",
    "            pred_i = preds[i]\n",
    "            X_i = X[i]\n",
    "            #output_img = rgb2gray(X_i)\n",
    "            #output_img2 = gray2rgb(output_img.copy())\n",
    "\n",
    "            #overlay = np.uint8(cm.viridis(pred_i) * 255)[:,:,:3]\n",
    "            #blended = overlay*alpha + output_img2 *(1-alpha) +  0\n",
    "            output_img = cv2.cvtColor(X_i, cv2.COLOR_RGB2GRAY)\n",
    "            output_img2 = cv2.cvtColor(output_img.copy(), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            overlay = np.uint8(cm.viridis(pred_i) * 255)[:,:,:3]\n",
    "            blended = cv2.addWeighted(overlay, alpha, output_img2, 1-alpha, 0, output_img)\n",
    "            #blended = overlay*alpha + output_img2 *(1-alpha) +  0\n",
    "            #blended = np.clip(blended, 0, 255)\n",
    "            plt.imsave(png_fname, blended)\n",
    "    return idx, output_thumbnail_pred\n",
    "\n",
    "all_batch_samples = []\n",
    "for offset in tqdm_notebook(list(range(0, n_samples, batch_size))):\n",
    "    all_batch_samples.append(all_samples.iloc[offset:offset+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(list(range(0, n_samples, batch_size)))\n",
    "\n",
    "output_thumbnail_preds = []\n",
    "output_thumbnail_idx = []\n",
    "with tqdm_notebook(total=total) as pbar:\n",
    "    with Pool(processes=8) as p:\n",
    "        results = p.imap_unordered(process_batch, enumerate(all_batch_samples))\n",
    "        for idx, result in results:\n",
    "            output_thumbnail_preds.append(result)\n",
    "            output_thumbnail_idx.append(idx)\n",
    "            pbar.update()\n",
    "    #output_thumbnail_pred = list(tqdm_notebook(p.imap(process_batch, all_batch_samples), total=total))\n",
    "        #for i, output_thumbnail_pred in enumerate(p.imap(process_batch, all_batch_samples)):\n",
    "        #    output_thumbnail_preds.append(output_thumbnail_pred)\n",
    "        #    pbar.update()\n",
    "\n",
    "output_thumbnail_preds = np.array(output_thumbnail_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def factors(n):    \n",
    "    return set(reduce(list.__add__, \n",
    "                ([i, n//i] for i in range(1, int(pow(n, 0.5) + 1)) if n % i == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10241 * 32/392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
